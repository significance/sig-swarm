---
title: Asides on Aggregation
keywords: 
    - swarm
    - user-sig
    - aggregation
---

### What should be aggregated?

When aggregating content, the following should be considered.

Which **salt** or salts have been used when determining the identifier.

The **index** and **indexing scheme** used to determine the identifier.

Which **account addresses** are to be included in the aggregation.

The encryption, encoding and schema of the underlying data.

### Aggregator Honesty

When data is agreggated, if the aggregation scheme produces deterministic results based on defined inputs, the aggreggation can be checked by many nodes.

An example of this could be when agreggating feeds for a certain epoch. Once the epoch has elapsed, all feeds generated in that time period should exist within the swarm. Gathering all of these feeds together and agreggating them into say, a json file using method which was guaranteed to always result in the same aggregation would then hash to produce the same address in Swarm.

As this version is confirmed to be true, a consensus is converged upon and nodes will then be able to forecast the reliability of aggregators.

### Search Indices

Although the above can quite easily produce efficient feeds of paginated data, there is an addition the need to provide search results from extremely large datasets, and the desire to benefit from fuzzy searches and other goodies provided by Elasticsearch, Solr and so on. If the dataset is sufficiently small, it may be possible to generate clientside search indices which can then be downloaded by individuals and queried on ones own machine. In the case this is not true, another approach is needed, a search service must be provided as an API and the veracity of this must be enforced by another layer of decentralised enforcement.

#### Incentive Schemes

A simplistic scheme would be that verifier nodes will do spot checks to check that the aggregators are being honest. In the event that the aggregator nodes are not honest, verifier nodes could benefit from some of their stake of BZZ. This could be verified with a cross checking and confirmation procedure that verifier nodes play against each other in return for stake. In the event the aggregator nodes are honest, they are rewarded with some BZZ from a total pool which is paid for as the aggregator nodes gives out access tokens for a single account to use it's API for a certain period of time.

#### Aggregation Methodologies

As has been mentioned, determining the feed space to aggregate is partially dependent on maintaining a list of the owner's Ethereum addresses.

Here we define some schemes by which these lists can be maintained in a decentralised manner.

##### Trojan Aggregator

1. A node runs a subscription to a certain topic and neighbourhood to listen for new messages.
2. Messages satisfying application level constraints predict addition to a list of owner feeds to be aggregated.

There is chicken and egg problem here. Who aggregates the aggregators? There must always be a centralised entrypoint to bootstrap any feed.

##### Beacon Aggregator [tbc]

This does not yet exist, it is intended to circumvent the chicken and egg problem mentioned above. 

1. Many nodes runs a subscription to a certain topic and neighbourhood to listen for new messages.
2. Messages satisfying application level constraints predict addition to a list of owner feeds to be aggregated.

This is anonymous and decentralised. It circumvents the problem because the only thing that is needed to be known is a neighbourhood and topic. Many nodes can then check/query this neighbourhood for the relevant beacon blocks and then build their own aggregations, there is no need for a centralised list of aggregators.

##### Blockchain Aggregator

- offloads feed enforcement policy to existing ethereum blockchain 
- simple, decentralised
- requires gas / blockchain
- complex inclusion algorithms
- aggregators could be centralised
- aggregators could be verified by centralised or blockchain enforced consensus strategy

##### SOC Aggregator

Aggregators create many ephemeral key pairs and advertise the associated key material. Aggregators then monitor the expected addresses to see if the SOC's have been created. The SOC's are then created with an agreed application schema by random participants including the address of the owner of the address they are using to create their feeds. Aggregators now have knowledge of feeds they are asked to aggregate.

The difficulty with this approach is that each aggregator may be aggregating different feeds. There would need to be an additional layer to determine a consensus of what feeds should be aggregated.

##### Simple Centralised Aggregator

It's simple but centralised.

##### Self Curated Opt In Aggregation

This is where each user curates their set of aggregations. The user could request a specific set of feeds to be aggregated and the aggregator would serve these up specifically for that individual. The individual then could request the same set from many different aggregators to get confirmation of it's veracity.

##### Top Down Protocol Aggregation

1. a user's address is sent to the aggregator using trojan
2. then the user's contribution to the feed is made at the SOC subspace indicated by the owner of the aggregated feed
3. the feed aggregation owner accepts the feed as part of the aggregation and creates SOC's to indicate acceptance
4. new users are notified of the new users feed and they are combined together 